services:
  # SQL Server 2022 untuk Data Warehouse
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: ptxyz_sqlserver
    hostname: sqlserver
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=${MSSQL_SA_PASSWORD}
      - MSSQL_PID=${MSSQL_PID}
    ports:
      - "1433:1433"
    volumes:
      - sqlserver_data:/var/opt/mssql
      - ./init-scripts:/docker-entrypoint-initdb.d
      - ./Dataset:/data
      - ./misi3:/scripts
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P '${MSSQL_SA_PASSWORD}' -Q 'SELECT 1'"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Apache Airflow untuk ETL Orchestration
  postgres:
    image: postgres:13
    container_name: ptxyz_postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: ptxyz_redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50

  airflow-webserver:
    build: ./airflow
    container_name: ptxyz_airflow_webserver
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
      - _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./Dataset:/opt/airflow/data
    depends_on:
      - postgres
      - redis
      - sqlserver
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build: ./airflow
    container_name: ptxyz_airflow_scheduler
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./Dataset:/opt/airflow/data
    depends_on:
      - postgres
      - redis
      - sqlserver
    networks:
      - dw_network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-worker:
    build: ./airflow
    container_name: ptxyz_airflow_worker
    command: celery worker
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./Dataset:/opt/airflow/data
    depends_on:
      - postgres
      - redis
      - sqlserver
    networks:
      - dw_network
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5

  # Apache Superset untuk Data Visualization
  superset:
    image: apache/superset:latest
    container_name: ptxyz_superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
    volumes:
      - superset_data:/app/superset_home
      - ./superset-config:/app/superset_config
    networks:
      - dw_network
    depends_on:
      - sqlserver
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username ${SUPERSET_USERNAME} --firstname Admin --lastname User --email ${SUPERSET_EMAIL} --password ${SUPERSET_PASSWORD} &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
      "

  # Jupyter Notebook untuk Data Analysis
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: ptxyz_jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=${JUPYTER_ENABLE_LAB}
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./Dataset:/home/jovyan/work/data
    networks:
      - dw_network
    command: start-notebook.sh --NotebookApp.token='${JUPYTER_TOKEN}' --NotebookApp.password=''

  # Grafana untuk Dashboard dan Monitoring
  grafana:
    image: grafana/grafana:latest
    container_name: ptxyz_grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=${GF_USERS_ALLOW_SIGN_UP}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - dw_network
    depends_on:
      - sqlserver

  # Metabase sebagai alternatif BI tool
  metabase:
    image: metabase/metabase:latest
    container_name: ptxyz_metabase
    ports:
      - "3001:3000"
    environment:
      - MB_DB_TYPE=${MB_DB_TYPE}
      - MB_DB_DBNAME=${MB_DB_DBNAME}
      - MB_DB_PORT=${MB_DB_PORT}
      - MB_DB_USER=${MB_DB_USER}
      - MB_DB_PASS=${MB_DB_PASS}
      - MB_DB_HOST=${MB_DB_HOST}
      - MB_DB_PASS=metabase
      - MB_DB_HOST=postgres_metabase
    volumes:
      - metabase_data:/metabase-data
    networks:
      - dw_network
    depends_on:
      - postgres_metabase

  postgres_metabase:
    image: postgres:13
    container_name: ptxyz_postgres_metabase
    environment:
      - POSTGRES_DB=metabase
      - POSTGRES_USER=metabase
      - POSTGRES_PASSWORD=metabase
    volumes:
      - postgres_metabase_data:/var/lib/postgresql/data
    networks:
      - dw_network

volumes:
  sqlserver_data:
  postgres_data:
  postgres_metabase_data:
  superset_data:
  grafana_data:
  metabase_data:

networks:
  dw_network:
    driver: bridge
