{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca11934",
   "metadata": {},
   "source": [
    "# PT XYZ Data Warehouse Analysis\n",
    "## Mining Operations Data Analysis\n",
    "\n",
    "This notebook provides analysis of PT XYZ mining operations data including:\n",
    "- Equipment usage patterns\n",
    "- Production metrics\n",
    "- Financial performance\n",
    "- Operational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f798d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files for analysis\n",
    "equipment_df = pd.read_csv('/home/jovyan/work/data/dataset_alat_berat_dw.csv')\n",
    "production_df = pd.read_csv('/home/jovyan/work/data/dataset_production.csv')\n",
    "transaction_df = pd.read_csv('/home/jovyan/work/data/dataset_transaksi.csv')\n",
    "\n",
    "print(f\"Equipment data: {len(equipment_df)} records\")\n",
    "print(f\"Production data: {len(production_df)} records\")\n",
    "print(f\"Transaction data: {len(transaction_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment Usage Analysis\n",
    "print(\"=== EQUIPMENT USAGE ANALYSIS ===\")\n",
    "print(\"\\nEquipment Types:\")\n",
    "print(equipment_df['equipment_type'].value_counts())\n",
    "\n",
    "print(\"\\nSites by Region:\")\n",
    "print(equipment_df.groupby('region')['site_name'].nunique())\n",
    "\n",
    "# Equipment efficiency\n",
    "equipment_df['efficiency'] = equipment_df['operating_hours'] / (equipment_df['operating_hours'] + equipment_df['downtime_hours']) * 100\n",
    "print(f\"\\nAverage Equipment Efficiency: {equipment_df['efficiency'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Analysis\n",
    "print(\"=== PRODUCTION ANALYSIS ===\")\n",
    "print(\"\\nMaterial Types:\")\n",
    "print(production_df['material_type'].value_counts())\n",
    "\n",
    "print(\"\\nProduction by Region:\")\n",
    "region_production = production_df.groupby('region')['produced_volume'].sum().sort_values(ascending=False)\n",
    "print(region_production)\n",
    "\n",
    "print(\"\\nEmployee Status Distribution:\")\n",
    "print(production_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Analysis\n",
    "print(\"=== FINANCIAL ANALYSIS ===\")\n",
    "print(\"\\nBudget vs Actual Cost Analysis:\")\n",
    "transaction_df['cost_variance_pct'] = ((transaction_df['actual_cost'] - transaction_df['budgeted_cost']) / transaction_df['budgeted_cost']) * 100\n",
    "\n",
    "print(f\"Total Budgeted Cost: ${transaction_df['budgeted_cost'].sum():,.2f}\")\n",
    "print(f\"Total Actual Cost: ${transaction_df['actual_cost'].sum():,.2f}\")\n",
    "print(f\"Total Variance: ${transaction_df['variance'].sum():,.2f}\")\n",
    "print(f\"Average Cost Variance: {transaction_df['cost_variance_pct'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258016c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Equipment efficiency by type\n",
    "equipment_eff = equipment_df.groupby('equipment_type')['efficiency'].mean().sort_values(ascending=False)\n",
    "axes[0,0].bar(equipment_eff.index, equipment_eff.values)\n",
    "axes[0,0].set_title('Average Equipment Efficiency by Type')\n",
    "axes[0,0].set_ylabel('Efficiency (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Production by material type\n",
    "material_prod = production_df.groupby('material_type')['produced_volume'].sum()\n",
    "axes[0,1].pie(material_prod.values, labels=material_prod.index, autopct='%1.1f%%')\n",
    "axes[0,1].set_title('Production Volume by Material Type')\n",
    "\n",
    "# Cost variance by project\n",
    "project_variance = transaction_df.groupby('project_name')['variance'].sum().sort_values()\n",
    "axes[1,0].barh(project_variance.index, project_variance.values)\n",
    "axes[1,0].set_title('Cost Variance by Project')\n",
    "axes[1,0].set_xlabel('Variance ($)')\n",
    "\n",
    "# Monthly production trend (if date data is available)\n",
    "production_df['date'] = pd.to_datetime(production_df['date'])\n",
    "monthly_prod = production_df.groupby(production_df['date'].dt.to_period('M'))['produced_volume'].sum()\n",
    "axes[1,1].plot(monthly_prod.index.astype(str), monthly_prod.values, marker='o')\n",
    "axes[1,1].set_title('Monthly Production Trend')\n",
    "axes[1,1].set_ylabel('Produced Volume')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection Test (when available)\n",
    "def connect_to_datawarehouse():\n",
    "    \"\"\"\n",
    "    Connect to the SQL Server data warehouse\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = (\n",
    "            \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "            \"SERVER=sqlserver,1433;\"\n",
    "            \"DATABASE=PTXYZ_DataWarehouse;\"\n",
    "            \"UID=sa;\"\n",
    "            \"PWD=PTXYZDataWarehouse2025;\"\n",
    "            \"TrustServerCertificate=yes;\"\n",
    "        )\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing database connection...\")\n",
    "conn = connect_to_datawarehouse()\n",
    "if conn:\n",
    "    print(\"✅ Database connection successful!\")\n",
    "    \n",
    "    # Query some basic stats from the data warehouse\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        'Equipment Usage' as table_name, COUNT(*) as record_count\n",
    "    FROM staging.EquipmentUsage\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Production' as table_name, COUNT(*) as record_count\n",
    "    FROM staging.Production\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Financial Transactions' as table_name, COUNT(*) as record_count\n",
    "    FROM staging.FinancialTransaction\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_stats = pd.read_sql(query, conn)\n",
    "        print(\"\\nData Warehouse Table Counts:\")\n",
    "        print(df_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"❌ Database connection failed - using CSV data only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f810b80",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "Based on the analysis above, here are the key insights from PT XYZ mining operations:\n",
    "\n",
    "1. **Equipment Efficiency**: Monitor equipment types with lower efficiency rates\n",
    "2. **Production Patterns**: Identify peak production periods and material demand\n",
    "3. **Cost Management**: Track projects with significant cost variances\n",
    "4. **Regional Performance**: Compare performance across different mining regions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Set up automated ETL pipeline in Airflow\n",
    "2. Create real-time dashboards in Grafana/Superset\n",
    "3. Implement alerting for equipment downtime\n",
    "4. Develop predictive maintenance models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
